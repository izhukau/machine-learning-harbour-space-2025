{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Midterm test and practice session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0. \n",
    "Please, formulate the supervised learning problem statement.\n",
    "\n",
    "Answer:\n",
    "\n",
    "So. We have dataset $\\{x_i, y_i\\}_{i=1}^n$ where $x_i$ - some features and $y_i$ - correct answers.\n",
    "\n",
    "Let's consider set of functions $h: X \\to Y$ and some \"loss\" function $\\ell(y, y')$ where $y_i' = h(x_i)$ - a prediction.\n",
    "\n",
    "And our goal is find such function $h$ so that minimaze the next equastion:\n",
    "\n",
    "$$\n",
    "h' = \\argmin_h \\mathbf{E}[\\ell(y, h(x))]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.\n",
    "\n",
    "What are regression and classification problems. Whatâ€™s the difference?\n",
    "\n",
    "\n",
    "Regression problem: We have dataset $\\{x_i, y_i\\}_{i=1}^n$ where $x_i$ - some features and $y_i$ - correct answers and $x_i \\in \\mathbf{R^d}$ and $y_i \\in \\mathbf{R}$. Let's $w \\to \\mathbf{R^{d}}$ is vector of weights and now our predictions will be:\n",
    "\n",
    "$$ y' = xw^T $$\n",
    "\n",
    "Let's write out our $x_i^T$ like columns in Matrix X and $y_i$ as column in matrix $Y$.\n",
    "\n",
    "And $Y' = Xw^T$\n",
    "\n",
    "And our loss function will be $\\ell(Y, Y')$. For instance MSE:\n",
    "\n",
    "$$\\frac{1}{n}||Xw^T - Y||^2_2$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Classification problems:\n",
    "Here we have dataset as well but we need to classify our objects (not to find some value like above). Here we have dataset $\\{x_i, y_i\\}_{i=1}^n$ where $x_i$ - some objects with futuers and $y_i$ - some classes (mostly just integer numbers). \n",
    "So we also have some loss function and out goal is to minimize value of this loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.\n",
    "Write down the LINEAR model for regression problem in matrix notation. What is Mean Squared Error (MSE) loss function? How can it be expressed?\n",
    "\n",
    "I wrote it above. So in matrix:\n",
    "write out our $x_i^T$ like columns in matrix $X \\in \\mathbf{R^{n \\times d}}$ and $y_i$ as column in matrix $Y \\in \\mathbf{R^{n}}$.\n",
    "\n",
    "And $Y' = Xw^T$ where $w$ is \"weight\" vector $\\to \\mathbf{R^{d}}$\n",
    "\n",
    "MSE:\n",
    "\n",
    "$$\\frac{1}{n}||Xw^T - Y||^2_2 = \\frac{1}{n}\\sum_{i=1}^n(x_iw^T - y_i)^2$$\n",
    "\n",
    "Actually I don't understand how correct my answer for this: \"How can it be expressed?\".\n",
    "\n",
    "So let's I write down solution to be sure:\n",
    "\n",
    "$$w = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.\n",
    "What is the difference between parameters and hyperparameters? Provide an example for linear models and decision trees.\n",
    "\n",
    "Parameters is value which occur in calculation. We don't setup its manually.\n",
    "Hyperparametrs is value which we setup manually befor the execution(start training) our model.\n",
    "\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. for linear models:\n",
    "\n",
    "Parameters: weights, bias\n",
    "\n",
    "Hyperparameters: $\\lambda$ in $L1$ and $L2$ regularization. \n",
    "\n",
    "2. for trees:\n",
    "\n",
    "Parameters: some features to split our tree into branches. value in leafs. \n",
    "\n",
    "Hyperparameters: max deep, min size split.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.\n",
    "Write down gradient descent step for linear model and MSE for one-dimensional case.\n",
    "\n",
    "So for one-dimensional case our linear model is: $y' = xw + b$.\n",
    "\n",
    "And MSE is $L(w) = \\frac{1}{n}\\sum_{i=1}^n(y - xw - b)^2$.\n",
    "\n",
    "Let's write down $\\frac{dL}{dw} = \\frac{1}{n}\\sum_{i=1}^n(\\frac{d}{dw}(y - xw - b)^2)$.\n",
    "\n",
    "Where $\\frac{d}{dw}(y - wx - b)^2 = \\frac{d}{dw}((wx)^2 - ywx + bwx) = (2wx - yx - bx) = x(2w - y - b)$\n",
    "\n",
    "And gradient descent here:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\mu \\frac{dL}{dw} = w^{(t)} - \\mu x(2w^{(t)} - y - b)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.\n",
    "What is validation? Cross validation? How is validation different from evaluation/test phase.\n",
    "\n",
    "Validation is phase after training and before testing (usually separete peace of dataset) to mostly find optimal hyperparametrs for our model. It is need to avoid \"overfitting\".\n",
    "\n",
    "Cross-validation is type of validation when we take different peace of the dataset and then combine them. For example:\n",
    "\n",
    "Let's figure out k-fold validation. Let's fix some k and divide our data into $k$ parts with the same size. Next let's make as \"validation set\" our first part among all k parts and the rest $\\frac{k-1}{k}n$ data will be training data. Create the training and then test our model in the validation set. After this let's make as \"validation part\" the second one of our k peaces and the rest like training part etc. We will have $\\frac{n}{k}$ measurments with some results. Let's just take average of these results as final. \n",
    "\n",
    "We need it to avoid overfitting and underfitting. \n",
    "\n",
    "But test phase is different phase. In this phase we test how accuracy our model and from these data make a conclusion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.\n",
    "What is regularization? How does L1 regularization differ from L2 for linear models?\n",
    "\n",
    "Regularization is way to limit our data. Make some \"specific\" view.\n",
    "\n",
    "\n",
    "\n",
    "1. L1 regularization is adding $\\lambda ||w||_1$ to our loss function. For exmpale in MSE:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}||Xw^T - Y||^2_2 + \\lambda ||w||_1.\n",
    "$$\n",
    "\n",
    "In this case we will \"filter\" our data. Unrelevant data will be very close to zero instead of more relevant. And of course here we will try to make our weights less.\n",
    "\n",
    "\n",
    "\n",
    "2. L2 regularization is adding $\\lambda ||w||^2_2$ to our loss function. For exmpale in MSE:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}||Xw^T - Y||^2_2 + \\lambda ||w||^2_2.\n",
    "$$\n",
    "\n",
    "In this case we will try to create our weights as littile as possible. Because where we create \"$||.||^2_2$\" each flactuation have a big cost. \n",
    "\n",
    "Worth noting that here $\\lambda$ is a hyperparametr. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.\n",
    "Why is it a good idea to normalize data before applying a linear model? What kinds of data normalization do you know?\n",
    "\n",
    "So it is good idea because each feature have different \"metric\".\n",
    "\n",
    "For instance: costs and height Here cost can be much more than height and in our linear model it will be worth more. But if we will normalize them before applying our model they will have the same \"priority\".\n",
    "\n",
    "kinds of data normalization:\n",
    "1. divide by mean(average value).\n",
    "2. divide by sum to make all values in [0, 1]. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.\n",
    "What are precision and recall metrics? Explain how you understand their meaning.\n",
    "\n",
    "So let's we have binary classifier and we need to determine our object like \"positive\" and \"negative\".\n",
    "\n",
    "Let's our model make all assumptions and have and we have some results. We can have 4 types:\n",
    "\n",
    "1. TP - amount of true positive determined (our model determine calss correctly)\n",
    "2. FP - amount of false positive determined\n",
    "3. TN - amount of true negative determined\n",
    "4. FN - amount of false negative determined\n",
    "\n",
    "So $precision = \\frac{TP}{TP + FP}$ - this is amount of ways when our model say \"positive\" and was right.  \n",
    "\n",
    "So $recall = \\frac{TP}{TP + FN}$ - this is amount of \"positive\" items that our model determined correctly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.\n",
    "How Random Forest is different from simple bagging? \n",
    "\n",
    "Simple bagging create several decision trees with \"replaced\" data (replaced data - we still have N data from N-sized dataset but several data was copied and some data was deleted). After this we will take avarage value among all our trees. \n",
    "\n",
    "Random forest also creates several decision trees and test it in \"replaced\" data. But in each split (in each node) we will take not all data for comprasion(for example we have p objects, let's take $\\sqrt{p}$ etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10\n",
    "\n",
    "How are base algorithms being trained in gradient boosting? Do we need to apply data normalization for gradient boosting?\n",
    "\n",
    "Our base algorithms trains step by step. Because of our answer is in the statement. We create gradient descent on likelihood of mistake (if I am remember correctly is called \"reminders\"). And such algorithm (gradient boosting) allows us \"fix\" our mistake and go towards negative gradient. So we create our assumption with help of previous measurments. \n",
    "\n",
    "Do we need to apply data normalization for gradient boosting?\n",
    "I think no. I guess our derivative will calculate incorrect and we will go not to the optimal direction. It will not gradient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optional discussion\n",
    "\n",
    "It is optional. If you want to share some suggestions or comments, use this area. We will contact you in private messages if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create exam on paper. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
