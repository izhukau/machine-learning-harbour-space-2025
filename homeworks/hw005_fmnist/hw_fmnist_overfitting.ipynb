{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nDsVMGiVgSq2"
   },
   "source": [
    "## NN Overfitting and How to Fix It\n",
    "\n",
    "##### Credits: [Radoslav Neychev](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3isBRG6PgSq6"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "def parse_pytorch_model(model, device):\n",
    "    H, W = 28, 28\n",
    "    sample_input = torch.randn(1, 1, H, W).to(device=device)\n",
    "    supported_modules = [\n",
    "        nn.Linear, nn.Flatten, nn.Dropout, nn.Dropout2d,\n",
    "        nn.ReLU, nn.LeakyReLU,\n",
    "        nn.Conv1d, nn.Conv2d,\n",
    "        nn.BatchNorm1d, nn.BatchNorm2d,\n",
    "        nn.MaxPool2d, nn.AvgPool2d,\n",
    "        nn.AdaptiveAvgPool2d, nn.AdaptiveMaxPool2d,\n",
    "    ]\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            module_idx = len(model_dict[\"layers\"])\n",
    "            if any([isinstance(module, instance) for instance in supported_modules]):\n",
    "                model_dict[\"layers\"].append(\n",
    "                    {\n",
    "                        \"index\": module_idx,\n",
    "                        \"layer\": {\n",
    "                            \"type\": module.__class__.__name__,\n",
    "                            \"params\": str(module),\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "    \n",
    "    model_dict = {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"layers\": []\n",
    "    }\n",
    "    hooks = []\n",
    "    model.apply(register_hook)\n",
    "    _ = model(sample_input)\n",
    "    # remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    return model_dict\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "\n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def get_accuracy(model, data_loader):\n",
    "    predicted_labels = []\n",
    "    real_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            y_predicted = model(batch[0].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "            real_labels.append(batch[1])\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    real_labels = torch.cat(real_labels)\n",
    "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
    "    return accuracy_score\n",
    "\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file `hw_overfitting_data_dict.npy` (link is on the task page), it will be used to generate submissions. Code below helps to load it (in case error occurs, download the file manually).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/girafe-ai/ml-course/blob/26s_harbour/homeworks/hw005_fmnist/hw_overfitting_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zeA6Q5-CgSq7"
   },
   "source": [
    "### Task #1: Creating and training a model (Separation)\n",
    "Let's go back to the simple image classification task which was discussed earlier. But now we will utilize the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. Let's use full dataset fot this task.\n",
    "\n",
    "__Your first task: implement the entire model training pipeline and reach accuracy baseline of $\\geq 88.5\\%$ on the test set.__\n",
    "\n",
    "There is no code for training the model in this task. There are only a few tests that will help you debug your solution. You may use notebook practice from previous class as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE_ID = 0  # change if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nPG1KbQAgl8b"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = (\n",
    "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "aYcL28OsgSq8",
    "outputId": "93aafa07-fb56-43bd-f928-918f45fe30e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 8')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_fmnist_data = FashionMNIST(\n",
    "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_fmnist_data = FashionMNIST(\n",
    "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f\"Image label: {_label}\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "S6jWRv1rgSq8"
   },
   "source": [
    "Define your model in the code block below. Please don't make it too complicated, don't use more than 4 layers (it can be even less actually). Your main goal is to train the model and exceed `accuracy` baseline of 88.5%.\n",
    "\n",
    "__ATTENTION, your model has to be defined in `model_task_1` variable. It should use batches of input tensors of shape (1, 28, 28).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BcyEFX-RgSq8"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class FashionMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "model_task_1 = FashionMLP().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAoLV4dkoy5M"
   },
   "source": [
    "Don't forget to move your model to the `device`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Xas9SIXDoxvZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMLP(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task_1.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6pLRWysggSq9"
   },
   "source": [
    "Local unit tests for sanity check are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qMQzo1ggSq9",
    "outputId": "c00008eb-ef88-4000-ce47-e8dedd26e061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].to(device)\n",
    "    y = random_batch[1].to(device)\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model_task_1(x)\n",
    "except Exception as e:\n",
    "    print(\"Something is wrong with the model\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
    "\n",
    "print(\"Everything seems fine!\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "suRmIPwIgSq9"
   },
   "source": [
    "Set up model hyperparameters for training. It's worth adjusting `learning rate` parameter as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YJnU14bdnZa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.5010\n",
      "Epoch [2/15], Loss: 0.3615\n",
      "Epoch [3/15], Loss: 0.3253\n",
      "Epoch [4/15], Loss: 0.3065\n",
      "Epoch [5/15], Loss: 0.2841\n",
      "Epoch [6/15], Loss: 0.2707\n",
      "Epoch [7/15], Loss: 0.2566\n",
      "Epoch [8/15], Loss: 0.2474\n",
      "Epoch [9/15], Loss: 0.2340\n",
      "Epoch [10/15], Loss: 0.2251\n",
      "Epoch [11/15], Loss: 0.2158\n",
      "Epoch [12/15], Loss: 0.2084\n",
      "Epoch [13/15], Loss: 0.2007\n",
      "Epoch [14/15], Loss: 0.1926\n",
      "Epoch [15/15], Loss: 0.1860\n",
      "Final Accuracy on Test Set: 89.41%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_task_1.parameters(), lr=0.001)\n",
    "num_epochs = 15 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_task_1.train()  \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model_task_1(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_data_loader):.4f}\")\n",
    "\n",
    "model_task_1.eval()  \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for images, labels in test_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model_task_1(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Final Accuracy on Test Set: {accuracy:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2zce7gt1gSq-"
   },
   "source": [
    "Don't forget to read the great [docs](https://pytorch.org/docs/stable/index.html) and [tutorials](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "usswrWYOgSq-"
   },
   "source": [
    "Evaluate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Xua3TVZHgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.93747\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "l9KEKXBxgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.8941\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyhmMobgSq_"
   },
   "source": [
    "Asserts for passing accuracy baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "OAIrURCEgSq_",
    "outputId": "7c983690-a92e-4693-89fb-7c86c002921a"
   },
   "outputs": [],
   "source": [
    "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
    "assert (\n",
    "    train_acc_task_1 >= 0.905\n",
    "), \"Test accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that code below expected your model to be defined in `model_task_1` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_task_1.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    \"train_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "    ),\n",
    "    \"test_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "    ),\n",
    "    \"model_task_1\": parse_pytorch_model(model_task_1, device),\n",
    "}\n",
    "\n",
    "with open(\"submission_dict_task_1.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_task_1.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2: Overfitting (Initiation)\n",
    "We're still working with the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. Now the task is to demonstrate model overfitting on the training set. In other words we need to demonstrate classification accuracy (not the loss value!), which is significantly higher for the training set in comparison with the testing set.\n",
    "\n",
    "Please note that in task #3 you will have to fix this model (reduce overfitting) using regularization, so don't overdo it!\n",
    "\n",
    "__Your second task: implement model training pipeline to demonstrate model ovefitting for the training set.__\n",
    "\n",
    "You can reuse some training code from above. There are a few tests further to help you check your solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this task model has to be defined in variable `model_task_2`.\n",
    "\n",
    "Don't use `Dropout` and `BatchNorm` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverfittingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OverfittingModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "\n",
    "model_task_2 = OverfittingModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Acc: 69.27%\n",
      "Epoch [2/40] | Train Acc: 75.16%\n",
      "Epoch [3/40] | Train Acc: 77.00%\n",
      "Epoch [4/40] | Train Acc: 78.23%\n",
      "Epoch [5/40] | Train Acc: 79.20%\n",
      "Epoch [6/40] | Train Acc: 79.65%\n",
      "Epoch [7/40] | Train Acc: 80.56%\n",
      "Epoch [8/40] | Train Acc: 80.88%\n",
      "Epoch [9/40] | Train Acc: 81.16%\n",
      "Epoch [10/40] | Train Acc: 81.46%\n",
      "Epoch [11/40] | Train Acc: 81.73%\n",
      "Epoch [12/40] | Train Acc: 82.02%\n",
      "Epoch [13/40] | Train Acc: 82.27%\n",
      "Epoch [14/40] | Train Acc: 82.30%\n",
      "Epoch [15/40] | Train Acc: 82.57%\n",
      "Epoch [16/40] | Train Acc: 82.83%\n",
      "Epoch [17/40] | Train Acc: 82.80%\n",
      "Epoch [18/40] | Train Acc: 82.87%\n",
      "Epoch [19/40] | Train Acc: 83.35%\n",
      "Epoch [20/40] | Train Acc: 83.35%\n",
      "Epoch [21/40] | Train Acc: 83.18%\n",
      "Epoch [22/40] | Train Acc: 83.70%\n",
      "Epoch [23/40] | Train Acc: 83.73%\n",
      "Epoch [24/40] | Train Acc: 83.60%\n",
      "Epoch [25/40] | Train Acc: 83.92%\n",
      "Epoch [26/40] | Train Acc: 84.10%\n",
      "Epoch [27/40] | Train Acc: 84.13%\n"
     ]
    }
   ],
   "source": [
    "model_task_2 = OverfittingModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_task_2.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_task_2.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_task_2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "model_task_2.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_task_2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * test_correct / test_total\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final Training Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Final Test Accuracy:     {test_acc:.2f}%\")\n",
    "print(f\"Overfitting Gap:         {train_acc - test_acc:.2f}%\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN architecture check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "layers_task_2 = []\n",
    "for element in parse_pytorch_model(model_task_2, device).get(\"layers\", []):\n",
    "    layer_name = element[\"layer\"][\"type\"]\n",
    "    assert \"dropout\" not in layer_name.lower(), \"Do not use Dropout in Task 2!\"\n",
    "    assert \"batchnorm\" not in layer_name.lower(), \"Do not use BatchNorm in Task 2!\"\n",
    "    layers_task_2.append(layer_name)\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.83685\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_2 = get_accuracy(model_task_2, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_2:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.8441\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_2 = get_accuracy(model_task_2, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_2:3.5}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that there is definitely overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Train accuracy must be higher than task accuracy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_acc_task_2 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m test_acc_task_2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain accuracy must be higher than task accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_acc_task_2 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.88\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain accuracy must be higher than 0.88\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     train_acc_task_2 \u001b[38;5;241m-\u001b[39m test_acc_task_2 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.04\u001b[39m\n\u001b[1;32m      5\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy should be at least 0.04 lower that train.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Train accuracy must be higher than task accuracy"
     ]
    }
   ],
   "source": [
    "assert train_acc_task_2 >= test_acc_task_2, \"Train accuracy must be higher than task accuracy\"\n",
    "assert train_acc_task_2 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
    "assert (\n",
    "    train_acc_task_2 - test_acc_task_2 >= 0.04\n",
    "), \"Test accuracy should be at least 0.04 lower that train.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note once again that for the code below to work your model has to be defined in `model_task_2` variable.\n",
    "\n",
    "Also note that `submission_dict` variable already has saved results from task #1. If it does not, reload them from the saved file to this variable before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_tasks_1_and_2.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict.update(\n",
    "    {\n",
    "        \"train_predictions_task_2\": get_predictions(\n",
    "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "        ),\n",
    "        \"test_predictions_task_2\": get_predictions(\n",
    "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "        ),\n",
    "        \"model_task_2\": parse_pytorch_model(model_task_2, device),\n",
    "    }\n",
    ")\n",
    "\n",
    "with open(\"submission_dict_tasks_1_and_2.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_tasks_1_and_2.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3: Fix the model (Return) \n",
    "We're still working with [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). Finally, let's fix overfitting issue for the model from task#2. By fixing it means to achieve classification accuracy difference of 0.015 (=1.5%) for training and testing accuracy using a new updated model.\n",
    "\n",
    "Note that model architecture in task#3 should not be very different from task#2! You can use Batchnorm, Dropout, data augmentation and also reduce the channel number. You can not reduce the number of layers!\n",
    "\n",
    "\n",
    "__Your third and final task: fix the model and/or training pipeline to fix overfitting.__\n",
    "\n",
    "Once again you can reuse model training code. As usual there are a few tests below for sanity check."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this task model has to be defined in variable `model_task_3`.\n",
    "\n",
    "Code below will also access `layers_task_2` variable. Define it if it was not defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert (\n",
    "    layers_task_2 is not None\n",
    "), \"Initialize layers_task_2 vairable which contains list of layers in task 2 model\"\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_task_2 = []\n",
    "for element in parse_pytorch_model(model_task_2, device).get(\"layers\", []):\n",
    "    layer_name = element[\"layer\"][\"type\"]\n",
    "    layers_task_2.append(layer_name)\n",
    "\n",
    "class RegularizedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        h = 192\n",
    "        self.noise_std = 0.05\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, h),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(h),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(h, h),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(h),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(h, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.noise_std > 0:   \n",
    "            x = x + torch.randn_like(x) * self.noise_std\n",
    "        x = self.flatten(x)\n",
    "        return self.stack(x)\n",
    "\n",
    "model_task_3 = RegularizedModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] Train Acc: 85.59%\n",
      "Epoch [2/15] Train Acc: 85.76%\n",
      "Epoch [3/15] Train Acc: 85.25%\n",
      "Epoch [4/15] Train Acc: 85.25%\n",
      "Epoch [5/15] Train Acc: 85.37%\n",
      "Epoch [6/15] Train Acc: 85.42%\n",
      "Epoch [7/15] Train Acc: 85.54%\n",
      "Epoch [8/15] Train Acc: 85.44%\n",
      "Epoch [9/15] Train Acc: 85.41%\n",
      "Epoch [10/15] Train Acc: 85.51%\n",
      "Epoch [11/15] Train Acc: 85.62%\n",
      "Epoch [12/15] Train Acc: 85.44%\n",
      "Epoch [13/15] Train Acc: 85.62%\n",
      "Epoch [14/15] Train Acc: 85.74%\n",
      "Epoch [15/15] Train Acc: 85.62%\n",
      "------------------------------\n",
      "Final Train Accuracy: 85.62%\n",
      "Final Test Accuracy:  86.53%\n",
      "Gap: -0.91%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_task_3.parameters(), lr=0.001, weight_decay=3e-4)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_task_3.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_task_3(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "model_task_3.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_task_3(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * test_correct / test_total\n",
    "gap = train_acc - test_acc\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final Train Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Final Test Accuracy:  {test_acc:.2f}%\")\n",
    "print(f\"Gap: {gap:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "layers_task_3 = []\n",
    "for element in parse_pytorch_model(model_task_3, device).get(\"layers\", []):\n",
    "    layer_name = element[\"layer\"][\"type\"]\n",
    "    layers_task_3.append(layer_name)\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for model_3_layer in layers_task_3:\n",
    "    model_2_layer = layers_task_2[idx]\n",
    "    if \"dropout\" not in model_3_layer.lower() and \"batchnorm\" not in model_3_layer.lower():\n",
    "        assert (\n",
    "            model_3_layer == model_2_layer\n",
    "        ), \"Models in tasks 2 and 3 must share the architecture except for Dropout and BatchNorm!\"\n",
    "        idx += 1\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.87262\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_3 = get_accuracy(model_task_3, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_3:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.8542\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_3 = get_accuracy(model_task_3, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_3:3.5}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if overfitting is still here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Train accuracy must be higher than 0.88",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_acc_task_3 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.88\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain accuracy must be higher than 0.88\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_acc_task_3 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.865\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy must be higher than 0.865\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_acc_task_3 \u001b[38;5;241m-\u001b[39m test_acc_task_3)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Train accuracy must be higher than 0.88"
     ]
    }
   ],
   "source": [
    "assert train_acc_task_3 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
    "assert train_acc_task_3 >= 0.865, \"Test accuracy must be higher than 0.865\"\n",
    "print(train_acc_task_3 - test_acc_task_3)\n",
    "assert (\n",
    "    train_acc_task_3 - test_acc_task_3 <= 0.015\n",
    "), \"Test accuracy should not be lower that train more than by 0.015\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that code below thinks that your model is defined in variable `model_task_3`.\n",
    "\n",
    "Also note that `submission_dict` variable should already have saved results from tasks #1 and #2. If it does not, reload them from the saved files before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict.update(\n",
    "    {\n",
    "        \"train_predictions_task_3\": get_predictions(\n",
    "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "        ),\n",
    "        \"test_predictions_task_3\": get_predictions(\n",
    "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "        ),\n",
    "        \"model_task_3\": parse_pytorch_model(model_task_3, device),\n",
    "    }\n",
    ")\n",
    "\n",
    "with open(\"submission_dict_final.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_final.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Xai8JL3tgSq_"
   },
   "source": [
    "### Submit the results for automatic grading\n",
    "Submit generated files to the corresponding contest tasks:\n",
    "    \n",
    "* `submission_dict_task_1.json` to Separation\n",
    "* `submission_dict_tasks_1_and_2.json` to Initiation\n",
    "* `submission_dict_final.json` to Return."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OtWnYAN_gSrA"
   },
   "source": [
    "The task is finished! Congrats!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
